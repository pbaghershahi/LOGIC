method: logic
llm_model: meta-llama/Meta-Llama-3.1-8B-Instruct
eval_step: 10
gamma: 0.5
gnn_batch_size: 64
gnn_dropout: 0.4
gnn_eval_step: 10
gnn_gamma: 0.5
gnn_h_dim: 64
gnn_lr: 0.01
gnn_num_epochs: 100
gnn_num_hid_layers: 3
gnn_decoder_type: linear
gnn_step_size: 50
gnn_type: gcn
gnn_weight_decay: 0.0005
gnn_with_last_dropout: true
gnn_task_mode: node
projector_lr: 0.0005
projector_step_size: 50
projector_num_epochs: 200
projector_weight_decay: 0.0
projector_gamma: 1.0
contrastive_temperature: 0.2
num_runs: 1
dataset: cora
seed: 
- 1234
normal_mode: na
max_num_frequent_words: 2000
dataset_sample_size: 1000
max_num_eval_nodes: 500
dataset_random_sampling: true
gnn_dataloader_num_workers: 0
neighborhood_max_hops: 2
use_bow: true
total_iters: 1
num_tokens: 5
llm_batch_size: 32
write_new_output: true
projector_objective_beta: 0.05
projector_contrastive_w: 1.0
projector_context_w: 1.0
projector_mutualinfo_w: 1.0
llm_node_batch_size: 5
save_generation_every: 10
train_test_split:
- 0.60
- 0.30